title: NPFL129, Lecture 8
class: title, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }
# SVR, Kernel Approximation,<br>Naive Bayes

## Milan Straka

### November 23, 2020

---
section: SVR
# SVM For Regression

![w=25%,f=right](svr_loss.svgz)

The idea of SVM for regression is to use an $ε$-insensitive error function
$$𝓛_ε\big(t, y(→x)\big) = \max\big(0, |y(→x) - t| - ε\big).$$

~~~
The primary formulation of the loss is then
$$C ∑_i 𝓛_ε\big(t_i, y(→x_i)\big) + \frac{1}{2} \|→w\|^2.$$

~~~
![w=25%,f=right](svr.svgz)

In the dual formulation, we require every training example to be within $ε$ of
its target, but introduce two slack variables $→ξ^-$, $→ξ^+$ to allow outliers. We therefore
minimize the loss
$$C ∑_i (ξ_i^- + ξ_i^+) + \tfrac{1}{2} \|→w\|^2$$
while requiring for every example $t_i - ε - ξ_i^- ≤ y(→x_i) ≤ t_i + ε + ξ_i^+$ for $ξ_i^- ≥ 0, ξ_i^+ ≥ 0$.

---
# SVM For Regression

The Lagrangian after substituting for $→w$, $b$, $→ξ^-$ and $→ξ^+$ is
$$L = ∑_i (a_i^+ - a_i^-) t_i - ε ∑_i (a_i^+ + a_i^-)
      - \frac{1}{2} ∑_i ∑_j (a_i^+ - a_i^-) (a_j^+ - a_j^-) K(→x_i, →x_j)$$

![w=40%,f=right](svr_example.svgz)

subject to
$$\begin{gathered}
  0 ≤ a_i^+, a_i^- ≤ C,\\
  ∑_i(a_i^+ - a_i^-) = 0.
\end{gathered}$$

~~~
The prediction is then given by
$$y(→z) = ∑_i (a_i^+ - a_i^-) K(→z, →x_i) + b.$$

---
section: KernelApprox
# Using RBF Kernel in Parametric Methods

The RBF kernel empirically works well, but can be used only in the kernel
methods (i.e., in the dual formulation, which is a non-parametric model), which
have time complexity superlinear with the size of the training data.

~~~
Therefore, several methods have been developed to allow using an approximation
of the RBF kernel in parametric models like logistic regression or MLP.

~~~
Generally, these methods define a mapping $→ψ:ℝ^D → ℝ^M$, generating $M$
features from a given input example, such that
$$K(→x, →z) ≈ →ψ(→x)^T →ψ(→z) = ∑_m →ψ_m(→x) →ψ_m(→z).$$

~~~
For a given example $→x$, the features $→ψ(→x)$ are then used as input to
a parametric classifier (or appended to other features we construct).

~~~
The hyperparameter $M$ affects the quality of the approximation and is usually
on the order of hundreds.

---
section: RFF
class: dbend
# Random Fourier Features

One way to approximate RBF kernel is Monte Carlo approximation of its Fourier
transform.

~~~
The Fourier transform of a real-valued integrable function $f$ is
$$f̂(w) ≝ \frac{1}{2π} ∫_{-∞}^∞ f(x) e^{-i x w} \d x,$$
where the $f̂(w)$ can be considered its _frequency spectrum_.

~~~
The transformation is invertible, and we can recover the original function as
$$f(x) = ∫_{-∞}^∞ f̂(w) e^{i x w} \d w.$$

---
class: dbend
# Random Fourier Features

Now consider a shift-invariant kernel $K(→x, →y) = k(→x - →y)$. If we knew its
frequency spectrum $p$, we could write it as
$$k(→x - →y) = ∫_{R^D} p(→w) e^{i →w^T (→x - →y)} \d →w,$$
~~~
which we can rewrite using $ξ(→x; →w) = e^{i →w^T →x}$ as
$$k(→x - →y) = ∫_{R^D} p(→w) e^{i →w^T (→x - →y)} \d →w = 𝔼_{→w ∼ p} \big[ξ(→x; →w) ξ(→y; →w)^*\big].$$

~~~
Therefore, $ξ(→x; →w) ξ(→y; →w)^*$ is an unbiased estimate of the kernel.

---
class: dbend
# Random Fourier Features

However, working with the complex numbers $ξ(→x; →w) = e^{i →w^T →x}$.
Nevertheless, considering that the kernel and the frequency spectrum is
real-valued, it is enough just to consider the real part of $ξ(→x; →w) ξ(→y; →w)^*$.

~~~
Recalling Euler formula stating that $e^{iθ} = \cos θ + i \sin θ$, the real part
of the $ξ$ product is $\cos(→w^T (→x-→y))$, and we would like to compute it
from $\cos(→w^T →x)$ and $\cos(→w^T →y)$, which are the real parts of
$ξ(→x; →w)$ and $ξ(→y; →w)$, respectively.

~~~
Remembering that $\cos(x±y) = \cos x \cos y ∓ \sin x \sin y$, we can rewrite
$\cos x\cos y$ as
$$\cos x \cos y = \tfrac{1}{2}\big(\cos(x - y) + \cos(x + y)\big).$$

~~~
In order to get rid of the last term, we introduce a bias $b$ sampled from
uniform distribution $U[0, 2π]$ and consider mappings
$$ψ(→x; →w, b) ≝ \sqrt 2 \cos(→w^T →x + b).$$

---
class: dbend
# Random Fourier Features

Combining the last two equations leads to
$$\begin{aligned}
  & 𝔼_{b∼U[0, 2π]} \big[\sqrt 2 \cos(→w^T →x + b) \sqrt 2 \cos(→w^T →y + b)\big] \\
 =& 𝔼_{b∼U[0, 2π]} \big[\cos(→w^T →x - →w^T →y) + \cos(→w^T →x + →w^T →y + 2b)\big] \\
 =& \cos(→w^T(→x - →y)),
\end{aligned}$$
where the last equation holds because the $\cos$ integrate to zero with respect
to $b$ (actually, range $[0, π]$ would be sufficient).

~~~
In order to decrease the variance of the estimator, we sample $M$ values
of $→w$ and $b$ and define
$$ψ_i(→x; →w_i, b_i) ≝ \sqrt{2/M} \cos(→w_i^T →x + b_i).$$

---
class: dbend
# Random Fourier Features

Lastly, we need the frequency spectrum of an RBF kernel.

~~~
It can be shown that for an RBF kernel with $γ=\tfrac{1}{2}$, the
frequency spectrum is the density function of the standard normal distribution,
$$p(→w) = 𝓝(→w; 0, 1).$$

~~~
To handle different values of $γ$, it is sufficient to suitably scale the
input features, which we can implement by scaling the sampled $→w$. It is
therefore straightforward to verify that using
$$→w ∼ \sqrt{2γ} 𝓝(0, 1)$$
results in an approximation of an RBF kernel with scale parameter $γ$.

~~~
The disadvantage of this approach is that we sample completely randomly,
not taking any data into consideration.

---
section: Nyström
# Nyström Approximation

A different approach to approximate an RBF kernel is to use a subset
of data as basis.

~~~
Assume that we have a sample of $M$ data $→x_1, …, →x_M$, denoting
$⇉K_{i,j} = K(→x_i, →x_j)$.

~~~
Our goal is to represent $K(→x, →y)$ as
$$K(→x, →y) ≈ ∑_{m=1}^M ψ_m(→x; →v_m) ψ_m(→y; →v_m)$$
by using linear mappings $ψ_m(→y; →v_m) = ∑_i →v_{m,i} K(→y, →x_i)$ with parameters $→v_m ∈ ℝ^M$.

~~~
If we denote the matrix with columns $→v_1, …, →v_M$ as $⇉V ∈ ℝ^{M×M}$, we can write
$$→ψ(→y; ⇉V) = ⇉V^T K(→y, →x_*),$$
where $K(→y, →x_*)$ is a vector of $K(→y, →x_1), …, K(→y, →x_M)$.

---
# Nyström Approximation

In order to construct our approximation, we choose $⇉V$ such that
our approximation is exact for all data $→x_1, …, →x_M$. Therefore,
it must hold that
$$⇉K_{i,j} = (⇉V^T ⇉K_i)^T ⇉V^T ⇉K_j.$$

~~~
We can rewrite the condition for all indices as $⇉K = ⇉K^T ⇉V ⇉V^T ⇉K$.

~~~
Therefore, we would like $⇉V$ to be something like $⇉K^{-1/2}$.

---
# Nyström Approximation

Because the kernel is a real symmetric matrix, it has an **eigenvalue decomposition**
$$⇉K = ⇉U ⇉D ⇉U^T,$$
where $⇉U$ is an orthogonal matrix and $⇉D$ is a diagonal one.

~~~
Therefore, we can take $⇉V = ⇉U ⇉D^{-1/2} ⇉U^T$, where
$$⇉D^{-1/2}_{i,i} = \begin{cases}
  0 &\mathrm{if}~D_{i,i} = 0,\\
  D_{i,i}^{-1/2}&\mathrm{otherwise}.
\end{cases}$$

~~~
It is then straightforward to show that the required equation holds.

~~~
The overall kernel is therefore approximated by computing the kernel values
of a given point and the chosen data subset, multiplied by $⇉V$. Empirically, the
approximation works usually better than random Fourier features, because it
concentrates more on the part of the space populated by the data.

---
class: dbend
# Nyström Approximation

On the previous page, we computed square roots of the diagonal matrix $⇉D$,
which exist only if the values on the diagonal are non-negative.

~~~
However, the matrix $⇉K$ is positive semi-definite for any kernel,
and a matrix is positive semi-definite iff all its eigenvalues are non-negative.

~~~
To show that the matrix $⇉K$ is positive semi-definite, we use the fact that
the corresponding kernel is defined as a scalar product of the feature map $→φ$:
$$\begin{aligned}
  →z^T ⇉K →z
  & = ∑\nolimits_{i,j} z_i →φ(→x_i)^T →φ(→x_j) z_j \\
  & = \left(∑\nolimits_i z_i →φ(→x_i)\right)^T \left(∑\nolimits_j z_j →φ(→x_j)\right) \\
  & = \left|\left|∑\nolimits_i z_i →φ(→x_i)\right|\right|^2 \\
  & ≥ 0.
\end{aligned}$$

---
section: TF-IDF
# Term Frequency – Inverse Document Frequency

To represent a document, we might consider it a **bag of words**, and create
a feature space with a dimension for every unique word. We can represent a word
in a document as:

- **binary indicators**: 1/0 depending on whether a word is present in
  a document or not;
~~~
- **term frequency (TF)**: relative frequency of a term in a document;
  $$\mathit{TF}(t) = \frac{\textrm{number of occurrences of $t$ in the document}}{\textrm{number of terms in the document}}$$
~~~
- **inverse document frequency (IDF)**: we could also represent a term using
  self-information of a probability of a random document containing it (therefore,
  terms with lower document probability have higher weights);
  $$\mathit{IDF}(t)
    = \log \frac{\textrm{number of documents}}{\textrm{number of documents containing $t$ }\big(\textrm{optionally} + 1)}
    = I\big(P(d ∋ t)\big)
  $$
~~~
- **TF-IDF**: empirically, product $\mathit{TF} ⋅ \mathit{IDF}$ is a feature
  reflecting quite well how important is a word to a document in a corpus
  (used by 83\% text-based recommender systems in 2015).

---
section: NaiveBayes
# Naive Bayes Classifier

Consider a discriminative classifier modelling probabilities
$$p(C_k|→x) = p(C_k | x_1, x_2, …, x_D).$$

~~~
We might use Bayes' theorem and rewrite it to
$$p(C_k|→x) = \frac{p(C_k) p(→x | C_k)}{p(→x)}.$$

~~~
The so-called **Naive Bayes** classifier assumes all $x_i$
are independent given $C_k$, so we can write
$$p(→x | C_k) = p(x_1 | C_k) p(x_2 | C_k, x_1) p(x_3 | C_k, x_1, x_2) ⋯ p(x_D | C_k, x_1, …)$$
as
$$p(C_k | →x) ∝ p(C_k) ∏_i p(x_i | C_k).$$

---
# Naive Bayes Classifier

There are several used naive Bayes classifiers, depending on the distribution
$p(x_i | C_k)$.

### Gaussian NB

The probability $p(x_i | C_k)$ is modeled as a normal distribution
$𝓝(μ_{i, k}, σ_{i, k}^2)$.

~~~
The parameters $μ_{i,k}$ and $σ_{i,k}^2$ are estimated directly from the data.
However, the variances are usually smoothed (increased) by a given constant $α$
to avoid too sharp distributions.

~~~
- The default value of $α$ in Scikit-learn is $10^{-9}$ times the largest variance
  of all features.

~~~
Gaussian NB is useful if we expect a continuous feature has normal distribution
for a given $C_k$.

---
# Gaussian Naive Bayes Example

![w=100%,v=middle](nb_gaussian.full.svgz)

---
# Multinomial Distribution

We have already discussed Bernoulli distribution and categorical distribution.

~~~
The **binomial distribution** is a generalization of Bernoulli distribution,
where we perform $n$ independent binary trials, each with fixed probability
of success. The binomial distribution gives the probability of a given
number of successes.

It is parametrized with a success probability $p ∈ [0, 1]$ and a number of
trials $n ∈ \{1, 2, …\}$, it is denoted as $B(n, p)$ and the probability of $k$
successes is $\binom{n}{k} p^k (1-p)^{n-k}$.

~~~
The **multinomial distribution** can be seen as a generalization of both the binomial
and categorical distributions. Assuming we are performing $n$ independent
trials, each with $k$ outcomes, where the outcomes have fixed probability, the
multinomial distribution gives the probability of every possible combination of
successes of every category.

It is parametrized with a probability distribution $→p$ and a number of trials
$n ∈ \{1, 2, …\}$, and the probability of $x_k$ outcomes of category $k$ is
$\binom{n}{x_1\,x_2\,…\,x_k} p_1^{x_1} p_2^{x_2} \cdots p_k^{x_k}$.

~~~
Both these distributions can be extended to a continuous numbers of
trials and successes using the _gamma function_ $Γ$.

---
# Naive Bayes Classifier

### Multinomial NB

When the distribution $p(→x | C_k)$ is multinomial, $p(→x | C_k) = \frac{(∑_i x_i)!}{∏_i x_i!}∏_i p_{i,k}^{x_i}$,
so the
$$\log p(C_k | →x) + \mathit{c} = \log p(C_k) + ∑\nolimits_i\log p_{i, k}^{x_i} = \log p(C_k) + ∑\nolimits_i x_i \log p_{i, k} = b + →x^T →w$$
is a linear model in the log space with $b = \log p(C_k)$ and $→w_i = \log p_{i, k}$.
The constant $c$ depends on $→x$ and its value is not needed neither for
estimation nor prediction.

~~~
Denoting $n_{i, k}$ as the sum of features $x_i$ for a class $C_k$, the
probabilities $p_{i, k}$ are usually estimated as
$$p_{i, k} = \frac{n_{i, k} + α}{∑_j n_{j, k} + αD}$$
where $α$ is a _smoothing_ parameter accounting for terms not appearing in any
document of class $C_k$ (we can view it as a _pseudocount_ given to every
term in every document).

---
# Naive Bayes Classifier

### Bernoulli NB

When the input features are binary, the $p(x_i | C_k)$ might also be a Bernoulli
distribution
$$p(x_i | C_k) = p_{i, k}^{x_i} ⋅ (1 - p_{i, k})^{(1-x_i)},$$
and as in the Multinomial NB case, we can write
$$\log p(C_k | →x) + c = \log p(C_k) + ∑\nolimits_i \big(x_i \log \tfrac{p_{i, k}}{1-p_{i,k}} + \log(1-p_{i,k})\big) = b + →x^T →w.$$
~~~
Similarly to the Multinomial NB, the probabilities are usually estimated as
$$p_{i, k} = \frac{\textrm{number of documents of class $k$ with nonzero feature $i$} + α}{\textrm{number of documents of class $k$} + 2α}.$$

~~~
The difference with respect to Multinomial NB is that Bernoulli NB explicitly
models also the _absence of terms_ by $(1-p_{i,k})$, while $p_{i,k}^0=1$ is used
in Multinomial NB. However, the cost is that the input features must be binary
(so for example TF-IDF cannot be used).

---
# Naive Bayes Example

![w=100%](nb_bernoulli.svgz)
![w=100%](nb_multinomial.svgz)
![w=100%](nb_gaussian.means.svgz)

---
section: GenerativeAndDiscriminative
# Naive Bayes Classifier as a Generative Model

Given that a Multinomial/Bernoulli NB fits $\log p(C_k, →x)$ as a linear model and
a logistic regression also fits $\log p(C_k | →x)$ as a linear model, naive Bayes and
logistic regression form a so-called **generative-discriminative** pair, where
the naive Bayes is a **generative** model, while logistic regression is
a **discriminative** model.

---
# Generative and Discriminative Models

So far, most of our models have been **discriminative**, modeling a _conditional
distribution_ $p(→t | →x)$ (predicting some output distribution). Empirically,
such models usually perform better in classification tasks, but because they do
not estimate the probability of $→x$, it might be difficult for them to
recognize outliers (out-of-distribution data).

~~~
On the other hand, the **generative** models estimate a _joint distribution_
$p(→t, →x)$, often by employing Bayes' theorem and estimating
$p(→x | →t) ⋅ p(→t)$. They therefore model the probability of the data being
generated by an outcome, and only transform it to $p(→t|→x)$ during prediction.

~~~
The term generative comes from a (theoretical) possibility of “generating”
random instances (either of $(→x, →t)$ or $→x$ given $→t$). However, just
being able to evaluate $p(→x | →t)$ does not necessarily mean there must be an
efficient procedure of actually sampling (generating) $→x$.

~~~
In recent years, generative modeling combined with deep neural networks created
a new family of _deep generative models_ like VAE or GAN, which can in fact
efficiently generate samples from $p(→x)$.

---
section: MAP
# Maximum A Posteriori Estimation

We already discussed maximum likelihood estimation
$$→w_\mathrm{MLE} = \argmax_→w p(⇉X; →w) = \argmax_→w p(⇉X | →w).$$

~~~
Instead, we may want to maximize _maximum a posteriori (MAP)_ point estimate:
$$→w_\mathrm{MAP} = \argmax_→w p(→w | ⇉X)$$

~~~
Using Bayes' theorem
$$p(→w | ⇉X) = \frac{p(⇉X | →w) p(→w)}{p(⇉X)},$$
we get
$$→w_\mathrm{MAP} = \argmax_→w p(⇉X | →w) p(→w).$$

---
# L2 Regularization as MAP

Another way to arrive at L2 regularization is to employ the MAP estimation
and assume that the prior probabilities $p(→w)$ of the parameter values (our
_preference_ among the models) is $𝓝(→w; 0, σ^2)$.

~~~
Then
$$\begin{aligned}
→w_\mathrm{MAP} &= \argmax_→w p(⇉X | →w) p(→w) \\
                &= \argmax_→w ∏\nolimits_{i=1}^N p(→x_i | →w) p(→w) \\
                &= \argmin_→w ∑\nolimits_{i=1}^N \big(-\log p(→x_i | →w) - \log p(→w)\big). \\
\end{aligned}$$

~~~
By substituting the probability of the Gaussian prior, we get
$$→w_\mathrm{MAP} = \argmin_→w ∑_{i=1}^N -\log p(→x_i | →w) {\color{gray} - \frac{1}{2} \log(2πσ^2)} + \frac{\|→w\|^2}{2σ^2}.$$
